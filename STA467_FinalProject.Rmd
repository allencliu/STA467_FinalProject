---
title: "Final Project"
author: "Allen Liu"
date: "2024-04-03"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(MASS)
library(splines)
library(glmnet)
library(randomForest)
library(GGally)
library(ggplot2)
library(corrplot)
library(MLeval)
```

## I. Introduction
This wine data set presents an intriguing opportunity for predictive modeling, focusing on physicochemical properties and sensory data of the red wine variants of the Portuguese "Vinho Verde" wine. Sourced from the UCI machine learning repository and detailed by [Cortez et al., 2009], this data set offers a classification task: determining wine quality based on a 0 to 10 scale.

Logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), regularization techniques (Lasso, Ridge, Elastic Net), and random forest will be used to analyze this data set. Each model is tuned and evaluated using cross-validation techniques for robustness and accuracy.

By setting a binary classification threshold for wine quality and leveraging advanced modeling approaches, the goal is to uncover the physiochemical attributes that differentiate red wines. The focus of this analysis extends beyond prediction; the aim is to understand the interplay of variables contributing to wine quality perception, thereby contributing to enology and predictive modeling.


## II. Exploratory Data Analysis (EDA)

```{r, echo=FALSE}
wine <- read.csv("winequality-red.csv", header=T, stringsAsFactors=F)
# head(wine)
```

```{r, echo=FALSE}
# Classify wines >= 7 as good (1) and not good < 7 (0)
wine$quality <- as.factor(ifelse(wine$quality >= 7, 'good', 'not_good'))
head(wine)
```
As part of the exploratory data analysis (EDA), the 'quality' variable will be modified into a binary format. This modification involves categorizing wines into two distinct groups: 'good' and 'not good.' The rationale behind this transformation is to simplify the analysis and modeling tasks by focusing on whether a wine is considered 'good' rather than its specific quality score.

An arbitrary cutoff point will be set at a quality score of 7 or higher, classifying wines with scores above this threshold as 'good' and the remaining wines as 'not good.' This decision is informed by domain knowledge and prior research indicating that wines with higher quality scores are generally perceived more favorably by consumers.

By converting the 'quality' variable into a binary format, it facilitates the identification of key factors that contribute to the perception of wine quality. Throughout this analysis, the binary 'quality' variable will be referred to, investigating the factors that distinguish 'good' wines from 'not good' ones, providing valuable insights for enology and predictive modeling tasks.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggpairs(wine)
```

From the ggpairs plot, some of the predictors exhibit right-skewed or non-normally distributed patterns. This observation is particularly notable in variables such as 'residual sugar,' 'chlorides,' 'free sulfur dioxide,' 'total sulfur dioxide,' and 'sulphates.' The right-skewed nature of these variables indicates a higher frequency of lower values with a tail stretching towards higher values. This skewness can impact the performance of certain statistical models that assume a normal distribution of data, potentially leading to biased estimates or inaccurate predictions. Therefore, the skewness in these predictors will be addressed through appropriate transformations or modeling techniques that can handle non-normal data effectively. This ensures that the modeling process accounts for the distributional characteristics of the predictors, ultimately enhancing the accuracy and reliability of the predictive models.

```{r, echo=FALSE}
# cor(wine[,-12])
corrplot(cor(wine[,-12]))
```
**Fixed Acidity vs. Citric Acid**: There is a strong positive correlation (approximately 0.67) between fixed acidity and citric acid. This indicates that wines with higher fixed acidity tend to have higher levels of citric acid as well.

**Fixed Acidity vs. Density**: Fixed acidity also shows a moderately positive correlation (around 0.67) with density. Wines with higher fixed acidity may thus tend to have higher densities.

**Volatile Acidity vs. Citric Acid**: There is a moderate negative correlation (about -0.55) between volatile acidity and citric acid. Wines with higher levels of volatile acidity are likely to have lower levels of citric acid.

**pH vs. Fixed Acidity and Citric Acid**: pH exhibits a strong negative correlation with fixed acidity (around -0.68) and a moderate negative correlation with citric acid (about -0.54). This suggests that wines with higher fixed acidity and lower citric acid content tend to have lower pH levels.

**Alcohol vs. Density**: Alcohol content shows a moderate negative correlation (approximately -0.50) with density. Wines with higher alcohol content may have lower densities.

It's important to note that while certain variables may exhibit strong correlations, the models used for analysis will still incorporate the full predictor set initially. This approach ensures that the models consider all available information and relationships among the predictors before any feature selection or removal is performed. Later in the analysis, the impact of removing predictors based on correlations or other criteria will be explored, and the performance of models with and without certain predictors will be compared. This comparative analysis will provide insights into the importance of individual predictors and their contribution to predictive modeling.

## III. Modeling Approach, Building, and Evaluation

The modeling approach employed for predicting wine quality involved utilizing a variety of machine learning algorithms. Specifically, the following models were trained and evaluated:

**1. Logistic Regression:**

  + The logistic regression model was trained using the "glm" method with repeated cross-validation (CV) performed using 10 folds and 10 repeats. The data was preprocessed by centering and scaling.
 
**2. Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA):**

  + LDA and QDA models were trained using their respective methods with the same repeated CV setup and preprocessing as logistic regression.
 
**3. Lasso, Ridge, and Elastic Net Regression:**

  + Lasso, Ridge, and Elastic Net models were trained using the "glmnet" method with repeated CV and preprocessing similar to the other models. Different regularization parameters (alpha and lambda) were tuned to optimize model performance.
 
**4. Random Forest:**

  + The Random Forest model was trained using the "rf" method with repeated CV, tuning the number of variables randomly sampled as candidates at each split (mtry) and setting the number of trees (ntree) to 100.

```{r, echo=FALSE}
set.seed(05122024)
# Logistic Regression
wine_logit <- train(quality ~ .,
                     data = wine,
                     method = "glm",
                     trControl = trainControl(method = "repeatedcv", 
                                              number = 10, 
                                              repeats = 10,
                                              savePredictions = TRUE, 
                                              summaryFunction = twoClassSummary, 
                                              classProbs = TRUE),
                     metric = "ROC",
                     preProcess=c("center","scale"))

```

```{r, echo=FALSE}
set.seed(05122024)
# LDA
wine_lda <- train(quality ~ .,
                     data = wine,
                     method = "lda",
                     trControl = trainControl(method = "repeatedcv", 
                                              number = 10, 
                                              repeats = 10, 
                                              savePredictions = TRUE, 
                                              summaryFunction = twoClassSummary, 
                                              classProbs = TRUE),
                     metric = "ROC",
                     preProcess=c("center","scale"))

# QDA
wine_qda <- train(quality ~ .,
                     data = wine,
                     method = "qda",
                     trControl = trainControl(method = "repeatedcv", 
                                              number = 10, 
                                              repeats = 10,
                                              savePredictions = TRUE, 
                                              summaryFunction = twoClassSummary, 
                                              classProbs = TRUE),
                     metric = "ROC",
                     preProcess=c("center","scale"))
```

```{r, echo=FALSE}
set.seed(05122024)
# Lasso/Ridge/Elastic

# Lasso
wine_lasso <- train(quality ~ .,
                    data = wine,
                    method="glmnet",
                    trControl=trainControl(method = "repeatedcv", 
                                           number = 10, 
                                           repeats = 10,
                                           savePredictions = TRUE, 
                                           summaryFunction = twoClassSummary, 
                                           classProbs = TRUE),
                    metric = "ROC",
                    tuneGrid=expand.grid(alpha=1,
                                         lambda=seq(0,0.1,by=0.01)),
                    preProcess=c("center","scale"))


# Ridge
wine_ridge <- train(quality ~ .,
                    data = wine,
                    method="glmnet",
                    trControl=trainControl(method = "repeatedcv", 
                                           number = 10, 
                                           repeats = 10,
                                           savePredictions = TRUE, 
                                           summaryFunction = twoClassSummary, 
                                           classProbs = TRUE),
                    metric = "ROC",
                    tuneGrid=expand.grid(alpha=0,
                                         lambda=seq(0,0.1,by=0.01)),
                    preProcess=c("center","scale"))

wine_elastic <- train(quality ~ .,
                    data = wine,
                    method="glmnet",
                    trControl=trainControl(method = "repeatedcv", 
                                           number = 10, 
                                           repeats = 10,
                                           savePredictions = TRUE, 
                                           summaryFunction = twoClassSummary, 
                                           classProbs = TRUE),
                    metric = "ROC",
                    tuneGrid=expand.grid(alpha=seq(0,0.1,by=0.01),
                                         lambda=seq(0,0.1,by=0.01)),
                    preProcess=c("center","scale"))

```

```{r, echo=FALSE}
set.seed(05122024)
# Decision Tree
wine_tree <- train(quality ~ .,
                   data=wine,
                   method="rf",
                   trControl=trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10,
                                          savePredictions = TRUE, 
                                          summaryFunction = twoClassSummary, 
                                          classProbs = TRUE),
                   metric = "ROC",
                   tuneGrid=data.frame(mtry=seq(1,11,by=1)),
                   ntree=100)
```

```{r, echo=FALSE}
models <- list(Logistic = wine_logit, LDA = wine_lda, QDA = wine_qda, Lasso = wine_lasso, Ridge = wine_ridge, Elastic = wine_elastic, "Random Forest" = wine_tree)
models <- models[order(names(models))]

evalm1 <- evalm(models, gnames = names(models), silent = TRUE, showplots = FALSE)

evalm1$roc

wine_results <- resamples(models)$values
summary(wine_results)
```


## IV. Results, Discussion, and Conclusion

### Results
**1. Model Performance**

The ROC curves reveal that the Random Forest model outperforms all others, demonstrating its superior fit for the wine data set. On the other hand, the Quadratic Discriminant Analysis (QDA) exhibits the lowest ROC curve, indicating comparatively weaker predictive performance. Among the remaining models, including Elastic Net, Lasso, LDA, Logistic Regression, and Ridge Regression, their ROC curves are closely clustered with negligible differences in performance. This suggests that while these models are competitive, the Random Forest model stands out as the most effective choice for accurately discriminating between wine quality.

```{r, echo=FALSE}
# varImp(wine_tree, scale = F)
plot(varImp(wine_tree, scale = F))
```

**2. Feature Importance Analysis**

The variable importance analysis reveals key insights into the importance of physicochemical features in predicting wine quality. Among the top-ranking features, alcohol is the most influential predictor with a high importance score of 57.95. This is followed closely by sulphates (44.12), volatile acidity (38.70), density (36.52), and citric acid (34.98). These features significantly contribute to the Random Forest model's ability to accurately classify wines as good or not good.

### Discussion
**3. Interpretation of Feature Importance**

The high importance of alcohol, sulphates, and volatile acidity highlights their crucial role in determining wine quality. These features likely capture aspects related to flavor profile, acidity levels, and alcohol content, which are known factors influencing wine quality.

**4. Specificity and Sensitivity**
Specificity and sensitivity are vital metrics in classification tasks. Specificity measures the ability of a model to correctly identify negative instances, indicating how well it avoids false positives. On the other hand, sensitivity quantifies the model's ability to detect positive instances accurately without missing actual positive cases, i.e. how well it avoids false negatives.

**Across the models tested:**
Random Forest exhibited a high sensitivity of up to 0.77, indicating its effectiveness in correctly identifying positive wine quality instances.
Specificity values were generally high across models, with Random Forest showing notable performance in correctly identifying negative instances. Sensitivity values, on the other hand, were generally low across models.

### Conclusion
**5. Key Findings**

The Random Forest model, driven by key features such as alcohol, sulphates, and volatile acidity, emerged as the top-performing model for wine quality prediction. Understanding the importance of these features provides important insights for wine producers and industry stakeholders.

In analyzing the low sensitivity values observed across some models, it becomes apparent that these models are better at predicting wines of lower quality (mediocre to low quality) compared to identifying wines of high quality. This phenomenon suggests that the models may excel at detecting negative instances (e.g., poor-quality wines) but struggle to identify positive instances (e.g., good-quality wines) with the same level of accuracy. One potential factor contributing to this imbalance in predictive performance is the nature of the dataset itself, which may be skewed towards containing more instances of mediocre to low-quality wines than instances of high-quality wines. This imbalance can lead to a higher emphasis on learning patterns associated with negative instances, thereby affecting the models' ability to generalize well to positive instances.

**6. Practical Implications**

The variable importance analysis has practical implications for wine production and quality improvement strategies. Producers can use these insights to optimize wine formulations, enhance quality control measures, and tailor products to meet consumer
preferences effectively.

In the context of wine quality assessment, where the focus often lies on identifying exceptional or high-quality wines, the low sensitivity values raise concerns about the models' effectiveness in precisely classifying such instances. Therefore, while the models may exhibit strong performance in terms of specificity (identifying non-good wines correctly), their lower sensitivity indicates a potential limitation in capturing and accurately predicting instances of high wine quality.


**7. Future Directions**

Future research endeavors may focus on exploring additional predictors or refining modeling techniques to further enhance predictive accuracy and deepen understanding of wine quality determinants.

Furthermore, addressing the challenge of having low sensitivities across the models requires strategies such as:

1. **Balancing the Dataset:** Collecting additional data or employing sampling techniques to balance the representation of different wine quality categories can help mitigate the effects of class imbalance.
2. **Adjusting Model Parameters:** Fine-tuning model parameters, such as adjusting class weights or using algorithms specifically designed for imbalanced datasets, can improve the models' sensitivity towards positive instances.
3. **Feature Engineering:** Incorporating domain knowledge and relevant features that better capture the characteristics of high-quality wines can enhance the models' ability to identify and predict such instances accurately.

By acknowledging and addressing these considerations, future iterations of the analysis can aim to improve the models' sensitivity specifically towards wines of high quality, aligning more closely with the practical objectives of wine quality assessment and decision-making in the industry.


## V. References and Appendices

```{r, echo=FALSE}
wine_logit_upsample <- train(quality ~ .,
                     data = wine,
                     method = "glm",
                     trControl = trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10,
                                              savePredictions = TRUE,
                                              summaryFunction = twoClassSummary,
                                              classProbs = TRUE,
                                              sampling = "up"),
                     metric = "ROC",
                     preProcess=c("center","scale"))


wine_lda_upsample <- train(quality ~ .,
                     data = wine,
                     method = "lda",
                     trControl = trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10,
                                              savePredictions = TRUE, 
                                              summaryFunction = twoClassSummary, 
                                              classProbs = TRUE,
                                              sampling = "up"),
                     metric = "ROC",
                     preProcess=c("center","scale"))

wine_qda_upsample <- train(quality ~ .,
                     data = wine,
                     method = "qda",
                     trControl = trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10,
                                              savePredictions = TRUE, 
                                              summaryFunction = twoClassSummary, 
                                              classProbs = TRUE,
                                              sampling = "up"),
                     metric = "ROC",
                     preProcess=c("center","scale"))

wine_lasso_upsample <- train(quality ~ .,
                    data = wine,
                    method="glmnet",
                    trControl=trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10,
                                           savePredictions = TRUE, 
                                           summaryFunction = twoClassSummary, 
                                           classProbs = TRUE,
                                           sampling = "up"),
                    metric = "ROC",
                    tuneGrid=expand.grid(alpha=1,
                                         lambda=seq(0,0.1,by=0.01)),
                    preProcess=c("center","scale"))

wine_ridge_upsample <- train(quality ~ .,
                    data = wine,
                    method="glmnet",
                    trControl=trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10,
                                           savePredictions = TRUE, 
                                           summaryFunction = twoClassSummary, 
                                           classProbs = TRUE,
                                           sampling = "up"),
                    metric = "ROC",
                    tuneGrid=expand.grid(alpha=0,
                                         lambda=seq(0,0.1,by=0.01)),
                    preProcess=c("center","scale"))

wine_elastic_upsample <- train(quality ~ .,
                    data = wine,
                    method="glmnet",
                    trControl=trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10,
                                           savePredictions = TRUE, 
                                           summaryFunction = twoClassSummary, 
                                           classProbs = TRUE,
                                           sampling = "up"),
                    metric = "ROC",
                    tuneGrid=expand.grid(alpha=seq(0,0.1,by=0.01),
                                         lambda=seq(0,0.1,by=0.01)),
                    preProcess=c("center","scale"))


wine_tree_upsample<- train(quality ~ .,
                   data=wine,
                   method="rf",
                   trControl=trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10,
                                          savePredictions = TRUE, 
                                          summaryFunction = twoClassSummary, 
                                          classProbs = TRUE,
                                          sampling = "up"), # Out-of-bag MSE
                   metric = "ROC",
                   tuneGrid=data.frame(mtry=seq(1,11,by=1)),
                   ntree=100)

# varImp(wine_tree, scale=FALSE)
# 
# plot(varImp(wine_tree, scale=FALSE))
# 
# wine_subset <- quality ~ alcohol + sulphates + volatile.acidity + density
# 
# wine_tree3 <- train(Class ~ alcohol + sulphates + volatile.acidity + density,
#                    data=up_train,
#                    method="rf",
#                    trControl=trainControl(method="cv", 
#                                            number = 10,
#                                           savePredictions = TRUE, 
#                                           summaryFunction = twoClassSummary, 
#                                           classProbs = TRUE), # Out-of-bag MSE
#                    metric = "ROC",
#                    tuneGrid=data.frame(mtry=seq(1,4,by=1)),
#                    ntree=100)
# 
# wine_tree4 <- train(formula(wine_subset),
#                    data=wine,
#                    method="rf",
#                    trControl=trainControl(method="cv", 
#                                            number = 10,
#                                           savePredictions = TRUE, 
#                                           summaryFunction = twoClassSummary, 
#                                           classProbs = TRUE), # Out-of-bag MSE
#                    metric = "ROC",
#                    tuneGrid=data.frame(mtry=seq(1,4,by=1)),
#                    ntree=100)

models2 <- list(Logistic = wine_logit_upsample, LDA = wine_lda_upsample, QDA = wine_qda_upsample, Lasso = wine_lasso_upsample, Ridge = wine_ridge_upsample, Elastic = wine_elastic_upsample, "Random Forest" = wine_tree_upsample)
models_upsample <- models2[order(names(models2))]

evalm_upsample <- evalm(models_upsample, gnames = names(models_upsample), silent = TRUE, showplots = FALSE)

evalm_upsample$roc

wine_results_upsample <- resamples(models_upsample)$values
summary(wine_results_upsample)
```

